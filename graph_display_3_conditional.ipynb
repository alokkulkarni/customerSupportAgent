{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from IPython.display import Image, display\n",
    "from state import State\n",
    "from utilities import create_tool_node_with_fallback\n",
    "\n",
    "from flightstool import fetch_user_flight_information\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "from agentassistant import (\n",
    "    Assistant,\n",
    "    part_3_assistant_runnable,\n",
    "    part_3_safe_tools,\n",
    "    part_3_sensitive_tools,\n",
    "    sensitive_tool_names,\n",
    ")\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "def user_info(state: State):\n",
    "    return {\"user_info\": fetch_user_flight_information.invoke({})}\n",
    "\n",
    "\n",
    "# NEW: The fetch_user_info node runs first, meaning our assistant can see the user's flight information without\n",
    "# having to take an action\n",
    "builder.add_node(\"fetch_user_info\", user_info)\n",
    "builder.add_edge(START, \"fetch_user_info\")\n",
    "builder.add_node(\"assistant\", Assistant(part_3_assistant_runnable))\n",
    "builder.add_node(\"safe_tools\", create_tool_node_with_fallback(part_3_safe_tools))\n",
    "builder.add_node(\n",
    "    \"sensitive_tools\", create_tool_node_with_fallback(part_3_sensitive_tools)\n",
    ")\n",
    "# Define logic\n",
    "builder.add_edge(\"fetch_user_info\", \"assistant\")\n",
    "\n",
    "\n",
    "def route_tools(state: State):\n",
    "    next_node = tools_condition(state)\n",
    "    # If no tools are invoked, return to the user\n",
    "    if next_node == END:\n",
    "        return END\n",
    "    ai_message = state[\"messages\"][-1]\n",
    "    # This assumes single tool calls. To handle parallel tool calling, you'd want to\n",
    "    # use an ANY condition\n",
    "    first_tool_call = ai_message.tool_calls[0]\n",
    "    if first_tool_call[\"name\"] in sensitive_tool_names:\n",
    "        return \"sensitive_tools\"\n",
    "    return \"safe_tools\"\n",
    "\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\", route_tools, [\"safe_tools\", \"sensitive_tools\", END]\n",
    ")\n",
    "builder.add_edge(\"safe_tools\", \"assistant\")\n",
    "builder.add_edge(\"sensitive_tools\", \"assistant\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "part_3_graph = builder.compile(\n",
    "    checkpointer=memory,\n",
    "    # NEW: The graph will always halt before executing the \"tools\" node.\n",
    "    # The user can approve or reject (or even alter the request) before\n",
    "    # the assistant continues\n",
    "    interrupt_before=[\"sensitive_tools\"],\n",
    ")\n",
    "\n",
    "try:\n",
    "    display(\n",
    "        Image(part_3_graph.get_graph(xray=True).draw_mermaid_png())\n",
    "    )\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import uuid\n",
    "from dbpopulation import update_dates, db\n",
    "from utilities import _print_event\n",
    "from langchain_core.messages.tool import ToolMessage\n",
    "\n",
    "# Update with the backup file so we can restart from the original place in each section\n",
    "db = update_dates(db)\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # The passenger_id is used in our flight tools to\n",
    "        # fetch the user's flight information\n",
    "        \"passenger_id\": \"3442 587242\",\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "tutorial_questions = [\n",
    "    \"Hi there, what time is my flight?\",\n",
    "    \"Am i allowed to update my flight to something sooner? I want to leave later today.\",\n",
    "    \"Update my flight to sometime next week then\",\n",
    "    \"The next available option is great\",\n",
    "    \"what about lodging and transportation?\",\n",
    "    \"Yeah i think i'd like an affordable hotel for my week-long stay (7 days). And I'll want to rent a car.\",\n",
    "    \"OK could you place a reservation for your recommended hotel? It sounds nice.\",\n",
    "    \"yes go ahead and book anything that's moderate expense and has availability.\",\n",
    "    \"Now for a car, what are my options?\",\n",
    "    \"Awesome let's just get the cheapest option. Go ahead and book for 7 days\",\n",
    "    \"Cool so now what recommendations do you have on excursions?\",\n",
    "    \"Are they available while I'm there?\",\n",
    "    \"interesting - i like the museums, what options are there? \",\n",
    "    \"OK great pick one and book it for my second day there.\",\n",
    "]\n",
    "\n",
    "\n",
    "_printed = set()\n",
    "# We can reuse the tutorial questions from part 1 to see how it does.\n",
    "for question in tutorial_questions:\n",
    "    events = part_3_graph.stream(\n",
    "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)\n",
    "    snapshot = part_3_graph.get_state(config)\n",
    "    while snapshot.next:\n",
    "        # We have an interrupt! The agent is trying to use a tool, and the user can approve or deny it\n",
    "        # Note: This code is all outside of your graph. Typically, you would stream the output to a UI.\n",
    "        # Then, you would have the frontend trigger a new run via an API call when the user has provided input.\n",
    "        try:\n",
    "            user_input = input(\n",
    "                \"Do you approve of the above actions? Type 'y' to continue;\"\n",
    "                \" otherwise, explain your requested changed.\\n\\n\"\n",
    "            )\n",
    "        except:\n",
    "            user_input = \"y\"\n",
    "        if user_input.strip() == \"y\":\n",
    "            # Just continue\n",
    "            result = part_3_graph.invoke(\n",
    "                None,\n",
    "                config,\n",
    "            )\n",
    "        else:\n",
    "            # Satisfy the tool invocation by\n",
    "            # providing instructions on the requested changes / change of mind\n",
    "            result = part_3_graph.invoke(\n",
    "                {\n",
    "                    \"messages\": [\n",
    "                        ToolMessage(\n",
    "                            tool_call_id=event[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "                            content=f\"API call denied by user. Reasoning: '{user_input}'. Continue assisting, accounting for the user's input.\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                config,\n",
    "            )\n",
    "        snapshot = part_3_graph.get_state(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
